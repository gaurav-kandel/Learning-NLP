{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "This notebook contains the exercise solutions for the Word Vectors Intuition Section of the Natural Language Processing Course. \n",
    "<br>\n",
    "<br>\n",
    "If you have any question refer to the Lecture **'Tutorial - How to complete the exercises'** in section 2 of the course.\n",
    "<br>\n",
    "<br>\n",
    "**NOTE: Depending on your Python version and library versions, your code may be correct but it may fail the asserts in the Validation cells - if your code matches the one on the solutions, don't worry and consider your exercise correct.**\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is the sentence we will use in this exercise\n",
    "sentence = 'Some cities in France are Paris Toulouse and Marseille'\n",
    "\n",
    "# Build the vocab of the sentence above\n",
    "# in list format - sort the elements of the list.\n",
    "# Call the returning object vocab\n",
    "# Before building the vocab apply lowercase to every word\n",
    "# in the sentence.\n",
    "\n",
    "vocab = list(set(sentence.lower().split(' ')))\n",
    "vocab.sort()\n",
    "\n",
    "# Build the co-ocurrence matrix for the sentence above for a \n",
    "# neighbor size = 2 (two neighbors on each side)\n",
    "\n",
    "# Store the Co-Ocurrence matrix as a numpy\n",
    "# object named co_ocurr\n",
    "\n",
    "# This is a hard exercise - take your time to\n",
    "# develop it!\n",
    "\n",
    "# I've done a nested loop implementation but you\n",
    "# can use any implementation you want as long\n",
    "# as you reach the final correct co-ocurrence matrix!\n",
    "\n",
    "# Hint: You main diagonal should have 0's\n",
    "import numpy as np\n",
    "co_ocurr = np.zeros([len(vocab),len(vocab)])\n",
    "\n",
    "neighbors = 2\n",
    "\n",
    "for i, element in enumerate(vocab):\n",
    "    split_sentence = sentence.lower().split(' ')\n",
    "    for index, word in enumerate(split_sentence):\n",
    "        if word == element:\n",
    "            first_el = 0 if index-neighbors < 0 else index - neighbors\n",
    "            last_el = len(sentence) if index+neighbors > len(sentence) else index + neighbors\n",
    "            \n",
    "            context = (\n",
    "                split_sentence[first_el:index]\n",
    "                +\n",
    "                split_sentence[index+1:last_el+1]\n",
    "            )\n",
    "            \n",
    "            for neighbor in context:\n",
    "                for j, pair_word in enumerate(vocab):\n",
    "                    if pair_word == neighbor:\n",
    "                        co_ocurr[i, j] += 1\n",
    "                        \n",
    "# Build the similarity matrix in our co_ocurr matrix \n",
    "# between every word (using cosine distance)\n",
    "\n",
    "# Store the values in a Pd.DataFrame with column\n",
    "# and index names with the names of the words\n",
    "# in the vocab with the name cosine_df\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "cosine_df = (\n",
    "    pd.DataFrame(\n",
    "        cosine_similarity(co_ocurr),\n",
    "        index=vocab,\n",
    "        columns=vocab\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a list with the sentences:\n",
    "# - Some cities in Portugal are Evora Lisboa and Porto\n",
    "# - Some cities in Spain are Madrid Barcelona and Valencia\n",
    "# - Some cities in the UK are London, Liverpool and Manchester\n",
    "\n",
    "# Call the object list_sentences\n",
    "\n",
    "# Additionally, build a new vocab based on all the words (lower case) \n",
    "# in the sentence list. Call the new object with the updated vocab\n",
    "# new_vocab - don't forget to sort it!\n",
    "\n",
    "list_sentences = [\n",
    "    'Some cities in Portugal are Evora Lisboa and Porto',\n",
    "    'Some cities in Spain are Madrid Barcelona and Valencia',\n",
    "    'Some cities in UK are London Liverpool and Manchester'\n",
    "]\n",
    "\n",
    "new_vocab = []\n",
    "for sentence in list_sentences:\n",
    "    split_sentence = sentence.lower().split(' ')\n",
    "    new_vocab.extend(split_sentence)\n",
    "\n",
    "new_vocab = list(set(new_vocab))\n",
    "\n",
    "new_vocab.sort()\n",
    "\n",
    "# Create a new co-ocurrence matrix (neighbor=2) based on the three sentences\n",
    "# above\n",
    "\n",
    "# call the object co_ocurr_multiple\n",
    "\n",
    "co_ocurr_multiple = np.zeros([len(new_vocab), len(new_vocab)])\n",
    "\n",
    "neighbors = 2\n",
    "\n",
    "for sentence in list_sentences:\n",
    "    for i, element in enumerate(new_vocab):\n",
    "        split_sentence = sentence.lower().split(' ')\n",
    "        for index, word in enumerate(split_sentence):\n",
    "            if word == element:\n",
    "                first_el = 0 if index - neighbors < 0 else index - neighbors\n",
    "                last_el = len(split_sentence) if index + neighbors >= len(split_sentence) else index + neighbors\n",
    "\n",
    "                context = (\n",
    "                    split_sentence[first_el:index]\n",
    "                    +\n",
    "                    split_sentence[index + 1:last_el + 1]\n",
    "                )\n",
    "\n",
    "                for neighbor in context:\n",
    "                    for j, pair_word in enumerate(new_vocab):\n",
    "                        if pair_word == neighbor:\n",
    "                            co_ocurr_multiple[i, j] += 1\n",
    "\n",
    "                            \n",
    "# Create the similarity matrix (in Pandas format) based on the co_ocurr_multiple\n",
    "# object\n",
    "# Use the cosine similarity metric\n",
    "# Call the object similarity_multiple\n",
    "\n",
    "similarity_multiple = pd.DataFrame(cosine_similarity(co_ocurr_multiple), index=new_vocab, columns=new_vocab)\n",
    "\n",
    "# Based on the similarity_multiple, what are the most\n",
    "# similar words to \"liverpool\"? Write the answer in list format, in alphabetical\n",
    "# order\n",
    "\n",
    "most_similar_words = list(\n",
    "    similarity_multiple.loc[similarity_multiple.liverpool > 0.49].index\n",
    ")\n",
    "\n",
    "_ = most_similar_words.pop(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation - Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your code is correct!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    vocab\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object vocab?')\n",
    "    \n",
    "try:\n",
    "    co_ocurr\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object co_ocurr?')\n",
    "    \n",
    "try:\n",
    "    cosine_df\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object cosine_df?')\n",
    "    \n",
    "try:\n",
    "    list_sentences\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object list_sentences?')\n",
    "    \n",
    "try:\n",
    "    new_vocab\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object new_vocab?')\n",
    "    \n",
    "try:\n",
    "    co_ocurr_multiple\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object co_ocurr_multiple?')\n",
    "    \n",
    "try:\n",
    "    most_similar_words\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object most_similar_words?')\n",
    "\n",
    "assert_1 = ['and',\n",
    "         'are',\n",
    "         'cities',\n",
    "         'france',\n",
    "         'in',\n",
    "         'marseille',\n",
    "         'paris',\n",
    "         'some',\n",
    "         'toulouse']\n",
    "\n",
    "assert_2 = [[0., 0., 0., 0., 0., 1., 1., 0., 1.],\n",
    "       [0., 0., 0., 1., 1., 0., 1., 0., 1.],\n",
    "       [0., 0., 0., 1., 1., 0., 0., 1., 0.],\n",
    "       [0., 1., 1., 0., 1., 0., 1., 0., 0.],\n",
    "       [0., 1., 1., 1., 0., 0., 0., 1., 0.],\n",
    "       [1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "       [1., 1., 0., 1., 0., 0., 0., 0., 1.],\n",
    "       [0., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
    "       [1., 1., 0., 0., 0., 1., 1., 0., 0.]]\n",
    "\n",
    "assert_3 = ['Some cities in Portugal are Evora Lisboa and Porto',\n",
    " 'Some cities in Spain are Madrid Barcelona and Valencia',\n",
    " 'Some cities in UK are London Liverpool and Manchester']\n",
    "\n",
    "assert_4 = ['and',\n",
    " 'are',\n",
    " 'barcelona',\n",
    " 'cities',\n",
    " 'evora',\n",
    " 'in',\n",
    " 'lisboa',\n",
    " 'liverpool',\n",
    " 'london',\n",
    " 'madrid',\n",
    " 'manchester',\n",
    " 'porto',\n",
    " 'portugal',\n",
    " 'some',\n",
    " 'spain',\n",
    " 'uk',\n",
    " 'valencia']\n",
    "\n",
    "assert_5 = [[0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
    "        1.],\n",
    "       [0., 0., 1., 0., 1., 3., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
    "        0.],\n",
    "       [1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "        1.],\n",
    "       [0., 0., 0., 0., 0., 3., 0., 0., 0., 0., 0., 0., 1., 3., 1., 1.,\n",
    "        0.],\n",
    "       [1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
    "        0.],\n",
    "       [0., 3., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 1., 3., 1., 1.,\n",
    "        0.],\n",
    "       [1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
    "        0.],\n",
    "       [1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
    "        0.],\n",
    "       [1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
    "        0.],\n",
    "       [1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
    "        0.],\n",
    "       [1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0.],\n",
    "       [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0.],\n",
    "       [0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0.],\n",
    "       [0., 0., 0., 3., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0.],\n",
    "       [0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "        0.],\n",
    "       [0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0.],\n",
    "       [1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0.]]\n",
    "\n",
    "assert_7 = ['barcelona', 'evora', 'lisboa', 'london', 'madrid', 'uk']\n",
    "\n",
    "assert(assert_1 == vocab)\n",
    "assert(np.array_equal(assert_2,co_ocurr))\n",
    "assert(pd.DataFrame(cosine_similarity(assert_2),index=assert_1,columns=assert_1).equals(cosine_df))\n",
    "assert(list_sentences == assert_3)\n",
    "assert(new_vocab == assert_4)\n",
    "assert(np.array_equal(assert_5,co_ocurr_multiple))\n",
    "assert(most_similar_words == assert_7)\n",
    "\n",
    "print('Your code is correct!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
