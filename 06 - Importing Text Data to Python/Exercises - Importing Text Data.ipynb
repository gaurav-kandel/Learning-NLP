{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "This notebook contains the exercises for the Importing Text Data Section of the Natural Language Processing Course. \n",
    "<br>\n",
    "<br>\n",
    "You can run your notebook on each cell that has a heading that starts with \"Exercise\" and validate your solution on each cell that has an heading that starts with \"Validation\".\n",
    "<br>\n",
    "<br>\n",
    "Don't forget that you code must in the bits with ```### YOUR CODE HERE``` and check if you create every object that the function expects!\n",
    "<br>\n",
    "There's an accompanying notebook that contains the solutions, if you need some help!\n",
    "<br>\n",
    "If you have any question refer to the Lecture **'Tutorial - How to complete the exercises'** in section 2 of the course.\n",
    "<br>\n",
    "<br>\n",
    "**NOTE: Depending on your Python version and library versions, your code may be correct but it may fail the asserts in the Validation cells - if your code matches the one on the solutions, don't worry and consider your exercise correct.**\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Read the file Tweets.csv stored in the folder\n",
    "# exercise data into a pandas dataframe\n",
    "# name the data frame tweets\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# Tokenize the first tweet of the tweets\n",
    "# data frame into words \n",
    "# using NLTK word tokenize\n",
    "# HINT: The tweet is in column\n",
    "# text\n",
    "\n",
    "# Save the tokenized tweet into\n",
    "# an object named tokenized_tweet\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# Read the tweets data into a list of dictionaries\n",
    "# using open() and csv.DictReader() functions\n",
    "# store the resulting object in a \n",
    "# tweets_dict object\n",
    "\n",
    "### YOUR CODE HERE\n",
    "        \n",
    "# Extract the tweet_id in the index 400 of the\n",
    "# tweets_dict dictionary\n",
    "# store it in an object named tweet_id_400 \n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation - Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your code is correct!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "with open('./exercise_data/Tweets.csv', mode='r', encoding='utf-8') as c_file:\n",
    "    c = csv.DictReader(c_file)\n",
    "    assert_3 = []\n",
    "    for row in c:\n",
    "        assert_3.append(row)\n",
    "\n",
    "try:\n",
    "    tweets\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object tweets?')\n",
    "    \n",
    "try:\n",
    "    tokenized_tweet\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object tokenized_tweet?')\n",
    "    \n",
    "try:\n",
    "    tweets_dict\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object tweets_dict?')\n",
    "    \n",
    "try:\n",
    "    tweet_id_400\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object tweet_id_400?')\n",
    "\n",
    "assert_1 = pd.read_csv('./exercise_data/Tweets.csv')  \n",
    "\n",
    "assert_2 = ['@', 'VirginAmerica', 'What', '@', 'dhepburn', 'said', '.']\n",
    "\n",
    "assert_4 = '568209028188233728'\n",
    "\n",
    "assert(tweets.equals(assert_1))\n",
    "assert(tokenized_tweet == assert_2)\n",
    "assert(tweets_dict == assert_3)\n",
    "assert(tweet_id_400 == assert_4)\n",
    "\n",
    "print('Your code is correct!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a request to the following wikipedia page\n",
    "# https://en.wikipedia.org/wiki/Pink_Floyd\n",
    "# using the requests library\n",
    "# store the response in a pink_floyd_wiki\n",
    "# named object\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# Soupify the pink_floyd_wiki object content using\n",
    "# Beautiful Soup - store the returning object\n",
    "# in a soup_pink_floyd_page\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# Use sent tokenizer to tokenize the sentences of the\n",
    "# Pink Floyd Page\n",
    "# Store the returning tokens in a wiki_token_pf named object\n",
    "# hint: use get_text() to get the text of the pink floyd\n",
    "# web page!\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# Store the first <h1> tag into a \n",
    "# h1_tag_pf object from the soup pink floyd\n",
    "# page\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# Store the first class = 'reference' in the\n",
    "# soup pink floyd page and store it in a \n",
    "# ref_class_pf object\n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation - Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your code is correct!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4 \n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "try:\n",
    "    pink_floyd_wiki\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object pink_floyd_wiki?')\n",
    "    \n",
    "try:\n",
    "    soup_pink_floyd_page\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object soup_pink_floyd_page?')\n",
    "    \n",
    "try:\n",
    "    wiki_token_pf\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object wiki_token_pf?')\n",
    "    \n",
    "try:\n",
    "    h1_tag_pf\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object h1_tag_pf?')   \n",
    "    \n",
    "try:\n",
    "    ref_class_pf\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object ref_class_pf?')\n",
    "\n",
    "assert_1 = requests.get('https://en.wikipedia.org/wiki/Pink_Floyd') \n",
    "\n",
    "assert(str(pink_floyd_wiki) == str(assert_1))\n",
    "assert(soup_pink_floyd_page == bs4.BeautifulSoup(assert_1.content, 'html.parser'))\n",
    "assert(wiki_token_pf == sent_tokenize(bs4.BeautifulSoup(assert_1.content, 'html.parser').get_text()))\n",
    "assert(h1_tag_pf == bs4.BeautifulSoup(assert_1.content, 'html.parser').find('h1'))\n",
    "assert(ref_class_pf == bs4.BeautifulSoup(assert_1.content, 'html.parser').find(class_='reference'))\n",
    "\n",
    "print('Your code is correct!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the following wikipedia page\n",
    "# https://en.wikipedia.org/wiki/A_Song_of_Ice_and_Fire\n",
    "# using the wikipedia library\n",
    "# Store the returning object in a \n",
    "# asoiaf_page named object\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# Extract the summary of the \n",
    "# asoiaf_page and store the returning object\n",
    "# in a summary_asoiaf_page\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# Retrieve the title of the asoiaf_page\n",
    "# and store it in an object called\n",
    "# asoiaf_page_title\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# Extract the most common 50 words from \n",
    "# the asoiaf page - keep stop words and punctuation\n",
    "# Lower the content of the asoiaf page\n",
    "# Store the resulting dictionary from FreqDist\n",
    "# with the top 50 words in an object\n",
    "# named top_50_words\n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation - Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your code is correct!\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    asoiaf_page\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object asoiaf_page?')\n",
    "    \n",
    "try:\n",
    "    summary_asoiaf_page\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object summary_asoiaf_page?')\n",
    "    \n",
    "try:\n",
    "    asoiaf_page_title\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object asoiaf_page_title?')\n",
    "    \n",
    "try:\n",
    "    top_50_words\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object top_50_words?')   \n",
    "\n",
    "assert_1 = wikipedia.page('A_Song_of_Ice_and_Fire')\n",
    "\n",
    "assert(asoiaf_page == assert_1)\n",
    "assert(summary_asoiaf_page == assert_1.summary)\n",
    "assert(asoiaf_page_title == assert_1.title)\n",
    "assert(top_50_words == nltk.FreqDist(\n",
    "    word_tokenize(\n",
    "        assert_1.content.lower()\n",
    "    )\n",
    ").most_common(50))\n",
    "\n",
    "print('Your code is correct!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('mlstanford': conda)",
   "language": "python",
   "name": "python37764bitmlstanfordcondae7887558efda4a86a663a84c2252353c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
