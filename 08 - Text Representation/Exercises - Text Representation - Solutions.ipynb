{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "This notebook contains the exercise solutions for the Text Representation Section of the Natural Language Processing Course. \n",
    "<br>\n",
    "<br>\n",
    "If you have any question refer to the Lecture **'Tutorial - How to complete the exercises'** in section 2 of the course.\n",
    "<br>\n",
    "<br>\n",
    "**NOTE: Depending on your Python version and library versions, your code may be correct but it may fail the asserts in the Validation cells - if your code matches the one on the solutions, don't worry and consider your exercise correct.**\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the txt file in exercise data \n",
    "# Called positive_movie_review.txt \n",
    "# into an object called positive_review\n",
    "with open('./exercise_data/positive_movie_review.txt', encoding='utf-8') as file:\n",
    "    positive_review = file.read()\n",
    "\n",
    "# Tokenize the positive_review \n",
    "# file into sentences using sent_tokenize\n",
    "# and store in a review_token object\n",
    "from nltk.tokenize import sent_tokenize\n",
    "review_token = sent_tokenize(positive_review)\n",
    "\n",
    "# Create a one-hot vectorized version of the\n",
    "# positive review and call the object \n",
    "# vector_positive_review\n",
    "# You should have a row for each sentence\n",
    "\n",
    "# Store the object in a DataFrame format\n",
    "# with the column names as the vocab\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "\n",
    "vector_positive_review = pd.DataFrame(\n",
    "    cv.fit_transform(review_token).todense(),\n",
    "    columns=cv.get_feature_names_out()\n",
    ")\n",
    "\n",
    "\n",
    "# Create a count vectorize version of the \n",
    "# review_token object and store it\n",
    "# in a dataframe called \n",
    "# vector_positive_review_count\n",
    "\n",
    "# Use the vocab as column names\n",
    "\n",
    "# Only use words that appear\n",
    "# in more than 1% of the corpus\n",
    "cv_count = CountVectorizer(binary=False, min_df=0.01)\n",
    "\n",
    "vector_positive_review_count = pd.DataFrame(\n",
    "    cv_count.fit_transform(review_token).todense(),\n",
    "    columns=cv_count.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation - Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "Did you create the object positive_review?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     \u001b[43mpositive_review\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'positive_review' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m     positive_review\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDid you create the object positive_review?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     review_token\n",
      "\u001b[1;31mNameError\u001b[0m: Did you create the object positive_review?"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv_count = CountVectorizer(binary=False, min_df=0.01)\n",
    "\n",
    "with open('./exercise_data/positive_movie_review.txt', encoding='utf-8') as f:\n",
    "    assert_1 = f.read()\n",
    "\n",
    "try:\n",
    "    positive_review\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object positive_review?')\n",
    "    \n",
    "try:\n",
    "    review_token\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object review_token?')\n",
    "    \n",
    "try:\n",
    "    vector_positive_review\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object vector_positive_review?')\n",
    "    \n",
    "try:\n",
    "    vector_positive_review_count\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object vector_positive_review_count?')\n",
    "\n",
    "assert_2 = sent_tokenize(assert_1)\n",
    "    \n",
    "assert(assert_1 == positive_review)\n",
    "assert(assert_2 == review_token)\n",
    "assert(pd.DataFrame(\n",
    "    cv.fit_transform(assert_2).todense(),\n",
    "    columns=cv.get_feature_names_out()\n",
    ").equals(vector_positive_review))\n",
    "assert(pd.DataFrame(\n",
    "    cv_count.fit_transform(assert_2).todense(),\n",
    "    columns=cv_count.get_feature_names_out()\n",
    ").equals(vector_positive_review_count))\n",
    "\n",
    "print('Your code is correct!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the txt file in exercise data \n",
    "# Called negative_movie_review.txt \n",
    "# into an object called negative_review\n",
    "with open('./exercise_data/negative_movie_review.txt', encoding='utf-8') as file:\n",
    "    negative_review = file.read()\n",
    "\n",
    "# Tokenize the negative_review \n",
    "# file into sentences using sent_tokenize\n",
    "# and store in a review_token_neg object\n",
    "from nltk.tokenize import sent_tokenize\n",
    "review_token_neg = sent_tokenize(negative_review)\n",
    "\n",
    "# Score the TF-IDF for the word killer in the\n",
    "# first sentence of the corpus tokenized above.\n",
    "\n",
    "# Remember that you have to:\n",
    "# - count the occurences of the word in the sentence\n",
    "# - check the presence of the word in the other sentences\n",
    "# - apply the formula we have learned in the lectures\n",
    "\n",
    "# Name the object tf_idf_killer_score\n",
    "count_killer = review_token_neg[0].count('killer')\n",
    "count_killers = sum(['killer' in review for review in review_token_neg])\n",
    "        \n",
    "import math\n",
    "tf_idf_killer_score = count_killer*(\n",
    "    math.log(\n",
    "        len(review_token_neg)\n",
    "        /\n",
    "        count_killers\n",
    "    )\n",
    ")\n",
    "\n",
    "# Use the TFIDF Vectorizer on the \n",
    "# review_token_neg object \n",
    "# and store the object in a sparse matrix format\n",
    "# called sparse_tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "sparse_tf = tf_idf.fit_transform(review_token_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your code is correct!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tftest = TfidfVectorizer()\n",
    "\n",
    "with open('./exercise_data/negative_movie_review.txt', encoding='utf-8') as f:\n",
    "    assert_1 = f.read()\n",
    "\n",
    "try:\n",
    "    negative_review\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object negative_review?')\n",
    "    \n",
    "try:\n",
    "    review_token_neg\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object review_token_neg?')\n",
    "    \n",
    "try:\n",
    "    tf_idf_killer_score\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object tf_idf_killer_score?')\n",
    "    \n",
    "try:\n",
    "    sparse_tf\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object sparse_tf?')\n",
    "\n",
    "assert_2 = sent_tokenize(assert_1)\n",
    "assert_3 = 1.792\n",
    "assert_4 = tftest.fit_transform(assert_2)\n",
    "\n",
    "assert(assert_1 == negative_review)\n",
    "assert(assert_2 == review_token_neg)\n",
    "assert(np.round(tf_idf_killer_score, 3) == assert_3)\n",
    "assert(np.allclose(assert_4.todense(),sparse_tf.todense()))\n",
    "\n",
    "print('Your code is correct!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy and load the spacy model en_core_web_md into\n",
    "# your Python environment\n",
    "\n",
    "# Save the model load into an object named nlp\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Define a function that takes two vectors and outputs the cosine similarity\n",
    "# between them. Use any method you would prefer \n",
    "\n",
    "# call the function compute_similarity\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_similarity(vector_1, vector_2):\n",
    "    sim_matrix = cosine_similarity([vector_1, vector_2])\n",
    "    return sim_matrix[0][1]\n",
    "\n",
    "\n",
    "# Extract the embeddings (using spacy) for the sentences: \n",
    "# sent1 = \"The cat sat on the mat.\"\n",
    "# sent2 = \"A feline is on the rug.\"\n",
    "\n",
    "# Save your vectors in a embed_1 and embed_2 object\n",
    "\n",
    "embed_1 = nlp('The cat sat on the mat.').vector\n",
    "embed_2 = nlp('A feline is on the rug.').vector\n",
    "\n",
    "# Use your function compute_similarity to calculate the similarity\n",
    "# between embed_1 and embed_2 \n",
    "\n",
    "# Save the similarity in an object named sim_1 \n",
    "sim_1 = compute_similarity(embed_1, embed_2)\n",
    "\n",
    "# Calculate the similarity of both sentences but\n",
    "# using the one-hot vectorizer approach\n",
    "\n",
    "# save the similarity in a sim_2 named object\n",
    "cv_1 = CountVectorizer(binary=True)\n",
    "\n",
    "sparse_matrix = cv_1.fit_transform(\n",
    "    ['The cat sat on the mat.',\n",
    "    'A feline is on the rug.']).todense()\n",
    "\n",
    "sim_2 = compute_similarity(sparse_matrix[0].tolist()[0], sparse_matrix[1].tolist()[0])\n",
    "\n",
    "\n",
    "# Answer the following questions: Based on the similarities using\n",
    "# one-hot vectorizer and document vectors (word2vec based), which \n",
    "# method extracts better \"meaning\" from the sentences?\n",
    "\n",
    "# The most appropriate method to capture meaning of the senteces is the \"word2vec\" based\n",
    "# as similarity is significantly higher than similarity calculated on top of a one-hot vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your code is correct!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Test for function existence\n",
    "try:\n",
    "    compute_similarity\n",
    "except NameError:\n",
    "    raise NameError('Did you define the function compute_similarity?')\n",
    "\n",
    "# Test for vector existence\n",
    "try:\n",
    "    embed_1\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object embed_1?')\n",
    "\n",
    "try:\n",
    "    embed_2\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object embed_2?')\n",
    "\n",
    "# Test for similarity calculations\n",
    "try:\n",
    "    sim_1\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object sim_1?')\n",
    "\n",
    "try:\n",
    "    sim_2\n",
    "except NameError:\n",
    "    raise NameError('Did you create the object sim_2?')\n",
    "\n",
    "# Actual values for assertions\n",
    "assert_sentences = ['The cat sat on the mat.', 'A feline is on the rug.']\n",
    "cv_assert = CountVectorizer(binary=True)\n",
    "sparse_matrix_assert = cv_assert.fit_transform(assert_sentences).todense()\n",
    "\n",
    "assert_embed_1 = nlp('The cat sat on the mat.').vector\n",
    "assert_embed_2 = nlp('A feline is on the rug.').vector\n",
    "assert_sim_1 = compute_similarity(assert_embed_1, assert_embed_2)\n",
    "assert_sim_2 = compute_similarity(sparse_matrix_assert[0].tolist()[0], sparse_matrix_assert[1].tolist()[0])\n",
    "\n",
    "# Making sure the values match the ones provided\n",
    "assert np.array_equal(embed_1, assert_embed_1)\n",
    "assert np.array_equal(embed_2, assert_embed_2)\n",
    "assert np.allclose(sim_1, assert_sim_1)\n",
    "assert np.allclose(sim_2, assert_sim_2)\n",
    "\n",
    "print('Your code is correct!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
